{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a83bf0d3-fac7-4204-be85-ce7d7a7e5318",
    "_uuid": "6c312474f35f552a05c0be7d86a3d2fd11d7026c"
   },
   "source": [
    "# Avito Demand Prediction Challenge - LightGBM Model\n",
    "\n",
    "## Introduction\n",
    "### The challenge\n",
    "When selling used goods online, a combination of tiny, nuanced details in a product description can make a big difference in drumming up interest.\n",
    "\n",
    "Avito, Russia’s largest classified advertisements website, is deeply familiar with this problem. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced).\n",
    "\n",
    "In [their fourth Kaggle competition](https://kaggle.com/c/avito-demand-prediction), Avito is challenging participants to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.\n",
    "\n",
    "The description of the data files from the [data page](https://www.kaggle.com/c/avito-demand-prediction/data):\n",
    "\n",
    "* train.csv - Train data.\n",
    "* test.csv - Test data. Same schema as the train data, minus deal_probability.\n",
    "* train_active.csv - Supplemental data from ads that were displayed during the same period as train.csv. Same schema as the train data, minus deal_probability.\n",
    "* test_active.csv - Supplemental data from ads that were displayed during the same period as test.csv. Same schema as the train data, minus deal_probability.\n",
    "* periods_train.csv - Supplemental data showing the dates when the ads from train_active.csv were activated and when they where displayed.\n",
    "* periods_test.csv - Supplemental data showing the dates when the ads from test_active.csv were activated and when they where displayed. Same schema as periods_train.csv, except that the item ids map to an ad in test_active.csv.\n",
    "* train_jpg.zip - Images from the ads in train.csv.\n",
    "* test_jpg.zip - Images from the ads in test.csv.\n",
    "* sample_submission.csv - A sample submission in the correct format.\n",
    "\n",
    "### LightGBM model\n",
    "\n",
    "In this notebook, we will train a [LightGBM](https://github.com/Microsoft/LightGBM) model to predict deal probabilities. LightGBM is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms. It is under the umbrella of the [DMTK](http://github.com/microsoft/dmtk) project of Microsoft.\n",
    "\n",
    "We break this notebook down into 4 steps:\n",
    "\n",
    "- [Step 1](#step1): Read in csv files\n",
    "- [Step 2](#step2): Engineer features: \n",
    "    - [categorical features](#step2a)\n",
    "    - [numerical features](#step2b)\n",
    "    - [text](#step2c)\n",
    "- [Step 3](#step3): Train and validate the model\n",
    "- [Step 4](#step4): Predict deal probabilities\n",
    "\n",
    "Then I'll end this notebook with [results and ideas for further improvement](#step5).\n",
    "\n",
    "First, let's import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "033852ed-2dd3-4bc3-bec1-ac3481ab175f",
    "_kg_hide-input": true,
    "_uuid": "af7422df064b7f16934cca9e86e8213bc9c667e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94a03718-1718-4981-88ef-532efcb435ee",
    "_uuid": "5d8e0a1a86fe5fbfa5162d3180d0b1ebea94f97d"
   },
   "source": [
    "<a id='step1'></a>\n",
    "## Load data\n",
    "\n",
    "We will load the training, testing and aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>activation_date</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>deal_probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b912c3c6a6ad</th>\n",
       "      <td>e00f8ff2eaf9</td>\n",
       "      <td>Свердловская область</td>\n",
       "      <td>Екатеринбург</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Постельные принадлежности</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кокоби(кокон для сна)</td>\n",
       "      <td>Кокон для сна малыша,пользовались меньше месяц...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>Private</td>\n",
       "      <td>d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>0.12789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2dac0150717d</th>\n",
       "      <td>39aeb48f0017</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>Самара</td>\n",
       "      <td>Для дома и дачи</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>Другое</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Стойка для Одежды</td>\n",
       "      <td>Стойка для одежды, под вешалки. С бутика.</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>Private</td>\n",
       "      <td>79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...</td>\n",
       "      <td>692.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ba83aefab5dc</th>\n",
       "      <td>91e2f88dd6e3</td>\n",
       "      <td>Ростовская область</td>\n",
       "      <td>Ростов-на-Дону</td>\n",
       "      <td>Бытовая электроника</td>\n",
       "      <td>Аудио и видео</td>\n",
       "      <td>Видео, DVD и Blu-ray плееры</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Philips bluray</td>\n",
       "      <td>В хорошем состоянии, домашний кинотеатр с blu ...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>Private</td>\n",
       "      <td>b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...</td>\n",
       "      <td>3032.0</td>\n",
       "      <td>0.43177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02996f1dd2ea</th>\n",
       "      <td>bf5cccea572d</td>\n",
       "      <td>Татарстан</td>\n",
       "      <td>Набережные Челны</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Автомобильные кресла</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автокресло</td>\n",
       "      <td>Продам кресло от0-25кг</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>286</td>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>Company</td>\n",
       "      <td>e6ef97e0725637ea84e3d203e82dadb43ed3cc0a1c8413...</td>\n",
       "      <td>796.0</td>\n",
       "      <td>0.80323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7c90be56d2ab</th>\n",
       "      <td>ef50846afc0b</td>\n",
       "      <td>Волгоградская область</td>\n",
       "      <td>Волгоград</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили</td>\n",
       "      <td>С пробегом</td>\n",
       "      <td>ВАЗ (LADA)</td>\n",
       "      <td>2110</td>\n",
       "      <td>ВАЗ 2110, 2003</td>\n",
       "      <td>Все вопросы по телефону.</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>Private</td>\n",
       "      <td>54a687a3a0fc1d68aed99bdaaf551c5c70b761b16fd0a2...</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>0.20797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user_id                 region              city  \\\n",
       "item_id                                                               \n",
       "b912c3c6a6ad  e00f8ff2eaf9   Свердловская область      Екатеринбург   \n",
       "2dac0150717d  39aeb48f0017      Самарская область            Самара   \n",
       "ba83aefab5dc  91e2f88dd6e3     Ростовская область    Ростов-на-Дону   \n",
       "02996f1dd2ea  bf5cccea572d              Татарстан  Набережные Челны   \n",
       "7c90be56d2ab  ef50846afc0b  Волгоградская область         Волгоград   \n",
       "\n",
       "             parent_category_name               category_name  \\\n",
       "item_id                                                         \n",
       "b912c3c6a6ad          Личные вещи  Товары для детей и игрушки   \n",
       "2dac0150717d      Для дома и дачи           Мебель и интерьер   \n",
       "ba83aefab5dc  Бытовая электроника               Аудио и видео   \n",
       "02996f1dd2ea          Личные вещи  Товары для детей и игрушки   \n",
       "7c90be56d2ab            Транспорт                  Автомобили   \n",
       "\n",
       "                                  param_1     param_2 param_3  \\\n",
       "item_id                                                         \n",
       "b912c3c6a6ad    Постельные принадлежности         NaN     NaN   \n",
       "2dac0150717d                       Другое         NaN     NaN   \n",
       "ba83aefab5dc  Видео, DVD и Blu-ray плееры         NaN     NaN   \n",
       "02996f1dd2ea         Автомобильные кресла         NaN     NaN   \n",
       "7c90be56d2ab                   С пробегом  ВАЗ (LADA)    2110   \n",
       "\n",
       "                              title  \\\n",
       "item_id                               \n",
       "b912c3c6a6ad  Кокоби(кокон для сна)   \n",
       "2dac0150717d      Стойка для Одежды   \n",
       "ba83aefab5dc         Philips bluray   \n",
       "02996f1dd2ea             Автокресло   \n",
       "7c90be56d2ab         ВАЗ 2110, 2003   \n",
       "\n",
       "                                                    description    price  \\\n",
       "item_id                                                                    \n",
       "b912c3c6a6ad  Кокон для сна малыша,пользовались меньше месяц...    400.0   \n",
       "2dac0150717d          Стойка для одежды, под вешалки. С бутика.   3000.0   \n",
       "ba83aefab5dc  В хорошем состоянии, домашний кинотеатр с blu ...   4000.0   \n",
       "02996f1dd2ea                             Продам кресло от0-25кг   2200.0   \n",
       "7c90be56d2ab                           Все вопросы по телефону.  40000.0   \n",
       "\n",
       "              item_seq_number activation_date user_type  \\\n",
       "item_id                                                   \n",
       "b912c3c6a6ad                2      2017-03-28   Private   \n",
       "2dac0150717d               19      2017-03-26   Private   \n",
       "ba83aefab5dc                9      2017-03-20   Private   \n",
       "02996f1dd2ea              286      2017-03-25   Company   \n",
       "7c90be56d2ab                3      2017-03-16   Private   \n",
       "\n",
       "                                                          image  image_top_1  \\\n",
       "item_id                                                                        \n",
       "b912c3c6a6ad  d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...       1008.0   \n",
       "2dac0150717d  79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...        692.0   \n",
       "ba83aefab5dc  b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...       3032.0   \n",
       "02996f1dd2ea  e6ef97e0725637ea84e3d203e82dadb43ed3cc0a1c8413...        796.0   \n",
       "7c90be56d2ab  54a687a3a0fc1d68aed99bdaaf551c5c70b761b16fd0a2...       2264.0   \n",
       "\n",
       "              deal_probability  \n",
       "item_id                         \n",
       "b912c3c6a6ad           0.12789  \n",
       "2dac0150717d           0.00000  \n",
       "ba83aefab5dc           0.43177  \n",
       "02996f1dd2ea           0.80323  \n",
       "7c90be56d2ab           0.20797  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./csv/train.csv\", index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "test = pd.read_csv(\"./csv/test.csv\", index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and merge aggregated features into training and testing dataframes\n",
    "af = pd.read_csv(\"csv/aggregated_features.csv\", index_col=False)\n",
    "train = train.merge(af, on=\"user_id\", how=\"left\")\n",
    "test = test.merge(af, on=\"user_id\", how=\"left\")\n",
    "\n",
    "agg_cols = list(af.columns)[1:]\n",
    "\n",
    "del af\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine training and testing data so that it's convenient when engineering features, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1503424, 19)\n",
      "Test shape: (508438, 19)\n",
      "All data shape: (2011862, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of the target column before merging\n",
    "y_train = train.deal_probability.copy()\n",
    "train.drop(\"deal_probability\", axis=1, inplace=True)\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "# Store the number of rows for each dataframe\n",
    "n_train_rows = train.shape[0]\n",
    "n_test_rows = test.shape[0]\n",
    "\n",
    "# Combine traing and testing data\n",
    "df = pd.concat([train, test], axis=0)\n",
    "\n",
    "# Remove unused features\n",
    "df.drop([\"activation_date\", \"image\"],axis=1,inplace=True)\n",
    "print(\"All data shape:\", df.shape)\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94a03718-1718-4981-88ef-532efcb435ee",
    "_uuid": "5d8e0a1a86fe5fbfa5162d3180d0b1ebea94f97d"
   },
   "source": [
    "<a id='step2'></a>\n",
    "## Engineer features\n",
    "<a id='step2a'></a>\n",
    "### Categorical features\n",
    "\n",
    "We will first engineer categorical features: filling NA values and encoding the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\"user_id\", \"region\",\"city\", \"parent_category_name\", \"category_name\", \"user_type\", \"image_top_1\", \"param_1\", \"param_2\", \"param_3\"]\n",
    "# Fill NA values for image_top_1 with -1\n",
    "df[\"image_top_1\"].fillna(-1, inplace=True)\n",
    "\n",
    "# Encode the features using LabelEncoder since we are using a decision tree model.\n",
    "# If we use other models, we should consider One-Hot-Encoding + PCA\n",
    "# https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col].fillna(\"unknown\")\n",
    "    df[col] = label_encoder.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2b'></a>\n",
    "### Numerical features\n",
    "\n",
    "We will move on to numerical features. \n",
    "- For prices, we will fill NA prices with the median value. Then we will take the natural logarithm of price for reasons described [here](https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va).  \n",
    "- For aggregated features, we will simply fill NA values with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"price\"].fillna(df.price.median(), inplace=True)\n",
    "df[\"price\"] = np.log1p(df[\"price\"])\n",
    "for col in agg_cols:\n",
    "    df[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2c'></a>\n",
    "### Text features\n",
    "\n",
    "Now we will engineer text features. In the cell below, we will create new text features by counting the number of words, characters, digits, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new text features took: 45.33 minutes\n"
     ]
    }
   ],
   "source": [
    "start_text_feat = time.time()\n",
    "\n",
    "text_feats = [\"description\", \"title\"]\n",
    "df[\"desc_punc\"] = df[\"description\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "for col in text_feats:\n",
    "    # Change text to lowercase and lemmatize text\n",
    "    df[col] = df[col].apply(lambda x: pre_process(x))\n",
    "    df[col] = df[col].fillna(\"unknown\")\n",
    "    df[col + \"_num_words\"] = df[col].apply(lambda x: len(x.split()))\n",
    "    df[col + \"_num_unique_words\"] = df[col].apply(lambda x: len(set(w for w in x.split())))\n",
    "    df[col + \"_words_vs_unique\"] = df[col + \"_num_unique_words\"] / df[col + \"_num_words\"] * 100 \n",
    "    df[col + \"_num_chars\"] = df[col].apply(lambda x: len(x))\n",
    "    df[col + \"_num_alphabets\"] = df[col].apply(lambda x: (x.count(r\"[a-zA-Z]\")))\n",
    "    df[col + \"_num_alphanumeric\"] = df[col].apply(lambda x: (x.count(r\"[A-Za-z0-9]\")))\n",
    "    df[col + \"_num_digits\"] = df[col].apply(lambda x: (x.count(\"[0-9]\")))\n",
    "    \n",
    "df[\"title_desc_len_ratio\"] = df[\"title_num_chars\"] / df[\"description_num_chars\"] * 100\n",
    "df[\"desc_punc_ratio\"] = df[\"desc_punc\"] / df[\"description_num_chars\"] *100\n",
    "\n",
    "print(\"Creating new text features took: %0.2f minutes\"%((time.time() - start_text_feat) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will vectorize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization runtime: 7.26 minutes\n"
     ]
    }
   ],
   "source": [
    "# Helper function to return a column from a dataframe\n",
    "def get_col(col_name): \n",
    "    return lambda x: x[col_name]\n",
    "\n",
    "russian_stop = set(stopwords.words(\"russian\"))\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": \"word\",\n",
    "    \"token_pattern\": r\"\\w{1,}\",\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": \"l2\",\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "\n",
    "vectorizer = FeatureUnion([\n",
    "        (\"description\", TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=15000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col(\"description\"))),\n",
    "        (\"title\", CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words = russian_stop,\n",
    "            preprocessor=get_col(\"title\")))\n",
    "    ])\n",
    "    \n",
    "start_vect=time.time()\n",
    "\n",
    "vectorizer.fit(df.to_dict(\"records\"))\n",
    "vectorized_features = vectorizer.transform(df.to_dict(\"records\"))\n",
    "\n",
    "# Store the names of vectorized text features in a list for use later with LightGBM\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Vectorization runtime: %0.2f minutes\"%((time.time() - start_vect) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out some information about `vectorized_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 6041)\t0.4543893833878289\n",
      "  (0, 7068)\t0.25929010686776116\n",
      "  (0, 7081)\t0.2746891748346552\n",
      "  (0, 7307)\t0.22512829318931382\n",
      "  (0, 9905)\t0.26460858705946444\n",
      "  (0, 9910)\t0.3905090765801592\n",
      "  (0, 12056)\t0.2866256844030417\n",
      "  (0, 12455)\t0.3570984005206791\n",
      "  (0, 14250)\t0.1846703379952123\n",
      "  (0, 14277)\t0.3658829184219402\n",
      "  (0, 709955)\t1.0\n",
      "  (0, 709956)\t1.0\n",
      "  (0, 709963)\t1.0\n",
      "  (0, 710044)\t1.0\n",
      "  (0, 1079815)\t1.0\n",
      "  (1, 2957)\t0.5775736475334812\n",
      "  (1, 3261)\t0.553098801207857\n",
      "  (1, 8510)\t0.39416011875467694\n",
      "  (1, 12917)\t0.45291080640739895\n",
      "  (1, 883789)\t1.0\n",
      "  (1, 1097960)\t1.0\n",
      "  (1, 1098177)\t1.0\n",
      "  (2, 2131)\t0.40594257312454046\n",
      "  (2, 2249)\t0.2500699308745464\n",
      "  (2, 4712)\t0.2697888109891143\n",
      "  :\t:\n",
      "  (2, 7826)\t0.3238878711826573\n",
      "  (2, 10946)\t0.18077023298841607\n",
      "  (2, 12313)\t0.36779397374107975\n",
      "  (2, 12499)\t0.0958742990789805\n",
      "  (2, 13118)\t0.29891956069067105\n",
      "  (2, 13364)\t0.15083932622521384\n",
      "  (2, 14175)\t0.12413745049681915\n",
      "  (2, 14207)\t0.14984499153463327\n",
      "  (2, 172218)\t1.0\n",
      "  (2, 349917)\t1.0\n",
      "  (2, 350266)\t1.0\n",
      "  (3, 6504)\t0.5553453927322807\n",
      "  (3, 10408)\t0.22051268524992526\n",
      "  (3, 10476)\t0.8018513892330027\n",
      "  (3, 457942)\t1.0\n",
      "  (4, 3156)\t0.314879894466968\n",
      "  (4, 3166)\t0.512748381722166\n",
      "  (4, 3545)\t0.4471705362789722\n",
      "  (4, 3554)\t0.5338940569818313\n",
      "  (4, 13145)\t0.3910695545618815\n",
      "  (4, 54551)\t1.0\n",
      "  (4, 59006)\t1.0\n",
      "  (4, 59017)\t1.0\n",
      "  (4, 529675)\t1.0\n",
      "  (4, 529695)\t1.0\n",
      "1242351\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorized_features))\n",
    "print(vectorized_features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's save `feature_names` and `df` that includes engineered features. This will save time in case we need to stop this notebook now, and resume it later. When we resume, we can just load these saved items without running the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop text columns\n",
    "df.drop(text_feats, axis=1, inplace=True)\n",
    "\n",
    "# Save df and feature_names so that we can quickly load them another time\n",
    "df.to_csv(\"csv/df_engineered_feats.csv\")\n",
    "\n",
    "PICKLED_FEATS = \"vectorized_feats.dat\"\n",
    "\n",
    "with open(PICKLED_FEATS, \"wb\") as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "# Uncomment below to load the saved features\n",
    "#with open(PICKLED_FEATS, \"rb\") as f:\n",
    " #   feature_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished feature engineering, we will stack dense features and sparse vectorized text features together. Before stacking them, we have to convert dense features to sparse matrices using `scipy.sparse.csr_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of featues:  1242383\n"
     ]
    }
   ],
   "source": [
    "# Convert dense features to sparse matrices and stack them with sparse vectorized text features\n",
    "X = hstack([csr_matrix(df[:n_train_rows]), vectorized_features[0:n_train_rows]])\n",
    "X_test = hstack([csr_matrix(df[n_train_rows:]), vectorized_features[n_train_rows:]])\n",
    "\n",
    "# Update feature_names with the column names of df\n",
    "feature_names = df.columns.tolist() + feature_names\n",
    "print(\"Number of featues: \", len(feature_names))\n",
    "\n",
    "del df, vectorizer, vectorized_features\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
